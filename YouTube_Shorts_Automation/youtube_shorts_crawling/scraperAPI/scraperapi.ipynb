{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c6ebf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "# Define the payload and headers for the request\n",
    "# shorts channel: https://www.youtube.com/watch?v=xEuH_5Ik92U\n",
    "# yt_shorts page: https://www.youtube.com/@%EC%95%84%EC%9D%B4%EB%8F%8C/shorts\n",
    "API_KEY = '3397e8a6a37e89b03b08750a27575df8'\n",
    "payload = {\n",
    "    'api_key': API_KEY,\n",
    "    'url': 'https://www.youtube.com/@%EC%95%84%EC%9D%B4%EB%8F%8C/shorts',\n",
    "    # 'render': 'true'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'x-sapi-render': 'true',\n",
    "    'x-sapi-instruction_set': '[{\"type\":\"scroll\",\"direction\":\"y\",\"value\":\"bottom\"}, {\"type\": \"wait\", \"value\": 2}]'\n",
    "    # , {\"type\": \"click\", \"selector\": {\"type\": \"text\",\"value\": \"Show transcript\"}}\n",
    "    # ,{\"type\":\"scroll\",\"direction\":\"y\",\"value\":\"bottom\"}, {\"type\": \"wait\", \"value\": 2} \n",
    "}\n",
    "# <div class=\"yt-spec-touch-feedback-shape yt-spec-touch-feedback-shape--touch-response\"\n",
    "# Make the GET request\n",
    "r = requests.get('https://api.scraperapi.com/', params=payload,  verify=False) # headers=headers,\n",
    "# Regular expression pattern to extract all instances of url paths like \"/shorts/xxxx\"\n",
    "url_pattern = r'\"url\":\"(/shorts/\\w+)\"'\n",
    "\n",
    "# Extract all URL paths using regex\n",
    "extracted_urls = re.findall(url_pattern, text)\n",
    "# Save the response text to a file\n",
    "with open('response_channel_shorts.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(r.text)\n",
    "\n",
    "print(\"Response saved to response_output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97ebe919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yt_title:  청각장애 팬을 감동시킨 스피드 ㅠ\n",
      "yt_shorts_description:  #ishowspeed #shorts\n",
      "yt_shorts_published:  2024-10-06 04\n",
      "yt_shorts_thumbnalil:  https://i.ytimg.com/vi/xEuH_5Ik92U/hq2.jpg?sqp=-oaymwE1CKgBEF5IVfKriqkDKAgBFQAAiEIYAHABwAEG8AEB-AG2CIACgA-KAgwIABABGH8gGSgXMA8=\\u0026rs=AOn4CLAr1jqDw2dLYSgK8pU5UBZfxZ7tPg\n",
      "yt_view:  3,309,087\n"
     ]
    }
   ],
   "source": [
    "# scraperAPI result filter\n",
    "def basic_shorts_info(text):\n",
    "    #shorts title\n",
    "    match_title = re.search(r'<title>(.*?) - YouTube</title>', r.text)\n",
    "    if match_title:\n",
    "        extracted_title = match_title.group(1)  # Get the content inside the <title>...</title> tags\n",
    "    else:\n",
    "        extracted_title = \"No title found\"\n",
    "    # shorts description\n",
    "    description_match = re.search(r'<meta name=\"description\" content=\"(.*?)\">', r.text)\n",
    "    if description_match:\n",
    "        extracted_description = description_match.group(1)  # Get the content inside the description meta tag\n",
    "    else:\n",
    "        extracted_description = \"No description found\"\n",
    "    # shorts total view \n",
    "    date_time_pattern = r'datePublished\" content=\"(\\d{4}-\\d{2}-\\d{2})T(\\d{2}):'\n",
    "    # Find the date and time\n",
    "    match_date = re.search(date_time_pattern, text)\n",
    "\n",
    "    if match_date:\n",
    "        extracted_datetime = f\"{match_date.group(1)} {match_date.group(2)}\"\n",
    "    else:\n",
    "        extracted_datetime = \"No match found\"\n",
    "\n",
    "    # Thumbnail \n",
    "    thumbnail_pattern = r'\"url\":\"(https?://[^\\s\"]+)\"'\n",
    "    # Extract the URL using regex\n",
    "    thumbnail_match = re.search(thumbnail_pattern, text)\n",
    "    if thumbnail_match:\n",
    "        extracted_thumbnalil = thumbnail_match.group(1)\n",
    "    else:\n",
    "        extracted_thumbnalil = \"No Thumbnail found\"\n",
    "\n",
    "    # Total view\n",
    "    view_pattern = r'\"viewCountEntityKey\":\"[^\"]+\",\"factoid\":\\{\"factoidRenderer\":\\{\"value\":\\{\"simpleText\":\"([\\d,]+)\"'\n",
    "    # Extract the number using regex only if viewCountEntityKey is present\n",
    "    view_match = re.search(view_pattern, text)\n",
    "    if view_match:\n",
    "        extracted_view = view_match.group(1)\n",
    "    else:\n",
    "        extracted_view = \"No number found\"\n",
    "        \n",
    "\n",
    "    return extracted_title, extracted_description, extracted_datetime, extracted_thumbnalil, extracted_view\n",
    "\n",
    "yt_title, yt_shorts_description, yt_shorts_published, yt_shorts_thumbnalil, yt_view = basic_shorts_info(r.text)\n",
    "print(\"yt_title: \", yt_title)\n",
    "print(\"yt_shorts_description: \", yt_shorts_description)\n",
    "print(\"yt_shorts_published: \", yt_shorts_published)\n",
    "print(\"yt_shorts_thumbnalil: \", yt_shorts_thumbnalil)\n",
    "print(\"yt_view: \", yt_view)\n",
    "yt_shorts_script = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7750826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "API_KEY = '3397e8a6a37e89b03b08750a27575df8'\n",
    "# List of URLs to test\n",
    "urls = [\n",
    "    'https://www.youtube.com/watch?v=xEuH_5Ik92U',\n",
    "    'https://www.youtube.com/watch?v=CPsCEfs3m8I',\n",
    "    'https://www.youtube.com/watch?v=jXUHFbNo5Vg'\n",
    "]\n",
    "\n",
    "# API key and headers\n",
    "headers = {\n",
    "    'x-sapi-render': 'true',\n",
    "    # 'x-sapi-instruction_set': '[{\"type\":\"scroll\",\"direction\":\"y\",\"value\":\"bottom\"}, {\"type\": \"wait\", \"value\": 2}]'\n",
    "    'x-sapi-instruction_set': '[{\"type\":\"loop\",\"for\":' + str(len(urls))+',\"instructions\":[{\"type\":\"scroll\",\"direction\":\"y\",\"value\":\"bottom\"}, {\"type\": \"wait\", \"value\": 2}]}]'\n",
    "}\n",
    "\n",
    "# Loop through each URL, send request, and save response to a file\n",
    "for i, url in enumerate(urls):\n",
    "    payload = {\n",
    "        'api_key': API_KEY,\n",
    "        'url': url,\n",
    "        'render': 'true'\n",
    "    }\n",
    "    \n",
    "    # Send the request\n",
    "    r = requests.get('https://api.scraperapi.com/', params=payload, headers=headers, verify=False)\n",
    "    \n",
    "    # Save the response to a file, naming it based on the URL index\n",
    "    filename = f'response_{i + 1}.txt'\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(r.text)\n",
    "    \n",
    "    print(f'Response for {url} saved to {filename}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0decd",
   "metadata": {},
   "source": [
    "### Daily 쇼츠 체크\n",
    "- 95000 채널 매일 확인 KST 23:30 진행\n",
    "- request 95000\n",
    "- channel_df: YT 채널 데이터\n",
    "- loading_df: 적제 쇼츠 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ce72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 channel list와 적재 데이터 loading\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "API_KEY = '3397e8a6a37e89b03b08750a27575df8'\n",
    "# 한국, 미국 데이터 9507개\n",
    "channel_df = pd.read_excel('us_korea_test.xlsx', sheet_name=\"us_kor\", encoding='utf-8')\n",
    "# 적재 DB: # -7d, -14d, -21d 만 필터링\n",
    "loading_df = pd.read_excel(\"loading_db.xlsx\",encoding='utf-8')\n",
    "today = datetime.today()\n",
    "loading_df['check_date'] = loading_df['check_date'].dt.date\n",
    "filter_dates = [today.date() - timedelta(days=7), today.date() - timedelta(days=14), today.date() - timedelta(days=21)]\n",
    "# Filter the dataframe based on check_date\n",
    "loading_df = loading_df[loading_df['check_date'].isin(filter_dates)]\n",
    "# channel_df 와 loading_df 교집합 데이터\n",
    "merged_df = pd.merge(loading_df, channel_df, on='channel_name', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ec5be",
   "metadata": {},
   "source": [
    "##### 기존 수집했던 쇼츠 재수집 -7d, -14d, -21d\n",
    "1. 처음 돌릴때 최신 기준으로 1개 수집 \n",
    "2. 기존 url 날짜에 맵핑이 되면 list에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c08ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# merge_df 후 각 채널 shorts url 확인 \n",
    "def collect_past_shorts(api_key, merged_db):\n",
    "    today = datetime.today()\n",
    "    for check_dt, channel_shorts_url in zip(merged_db['check_date'], merged_db['yt_fixed_url']):\n",
    "        if check_dt==(today.date() - timedelta(days=14)) | check_dt==(today.date() - timedelta(days=21)):\n",
    "            payload = {\n",
    "                'api_key': api_key,\n",
    "                'url': channel_shorts_url\n",
    "            }\n",
    "            # Send the request\n",
    "            r = requests.get('https://api.scraperapi.com/', params=payload, headers=headers, verify=False)\n",
    "            shorts_url_pattern = r'\"url\":\"(/shorts/\\w+)\"'\n",
    "\n",
    "            # Extract all URL paths using regex\n",
    "            extracted_urls = re.findall(shorts_url_pattern, r.text)\n",
    "\n",
    "        # -7d에는 comments 추가 수집\n",
    "        elif check_dt==(today.date() - timedelta(days=7)): \n",
    "            payload = {\n",
    "                'api_key': API_KEY,\n",
    "                'url': 'https://www.youtube.com/watch?v=xEuH_5Ik92U',\n",
    "                'render': 'true'\n",
    "            }\n",
    "\n",
    "            headers = {\n",
    "                'x-sapi-render': 'true',\n",
    "                'x-sapi-instruction_set': '[{\"type\":\"scroll\",\"direction\":\"y\",\"value\":\"bottom\"}, {\"type\": \"wait\", \"value\": 2}]'\n",
    "                # , {\"type\": \"click\", \"selector\": {\"type\": \"text\",\"value\": \"Show transcript\"}}\n",
    "                # ,{\"type\":\"scroll\",\"direction\":\"y\",\"value\":\"bottom\"}, {\"type\": \"wait\", \"value\": 2} \n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf44e5",
   "metadata": {},
   "source": [
    "##### channel shorts url input으로 들어갈시 채널 url 필터링\n",
    "1. 최신 기준으로 맵핑이 되지 않는 경우 최근 3개 url수집 \n",
    "2. 기존 url 맵핑이 되면 list에 추가하여 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b517bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check_date: merged_df의 check_date\n",
    "def scanning_preprocessing(channel_shorts_url, api_key, check_date):\n",
    "    today = datetime.today()\n",
    "    \n",
    "    # -14d, -21d의 경우 좋아요, 커멘트수, view 수의 증가만 확인 진행\n",
    "    if check_date==(today.date() - timedelta(days=14)) | check_date==(today.date() - timedelta(days=21)):\n",
    "        payload = {\n",
    "            'api_key': api_key,\n",
    "            'url': channel_shorts_url\n",
    "        }\n",
    "        # Send the request\n",
    "        r = requests.get('https://api.scraperapi.com/', params=payload, headers=headers, verify=False)\n",
    "        shorts_url_pattern = r'\"url\":\"(/shorts/\\w+)\"'\n",
    "\n",
    "        # Extract all URL paths using regex\n",
    "        extracted_urls = re.findall(shorts_url_pattern, r.text)\n",
    "        # if extracted_urls loading_df\n",
    "        # extracted_urls\n",
    "\n",
    "    # -7d에는 comments 추가 수집\n",
    "    elif check_date==(today.date() - timedelta(days=7)): \n",
    "        payload = {\n",
    "            'api_key': API_KEY,\n",
    "            'url': 'https://www.youtube.com/watch?v=xEuH_5Ik92U',\n",
    "            'render': 'true'\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            'x-sapi-render': 'true',\n",
    "            'x-sapi-instruction_set': '[{\"type\":\"scroll\",\"direction\":\"y\",\"value\":\"bottom\"}, {\"type\": \"wait\", \"value\": 2}]'\n",
    "            # , {\"type\": \"click\", \"selector\": {\"type\": \"text\",\"value\": \"Show transcript\"}}\n",
    "            # ,{\"type\":\"scroll\",\"direction\":\"y\",\"value\":\"bottom\"}, {\"type\": \"wait\", \"value\": 2} \n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 매일 전체 채널 필터링\n",
    "# - 신규 쇼츠 데이터 발견시 list에 추가해서 comments까지 확인\n",
    "# - 과거 데이터가 없을 수 있으니 만약 하나도 매칭이 안되면 3개 수집, 있다면 그거 앞에꺼를 추가수집\n",
    "def scanning_yt_shorts(api_key, channel_db, loading_db):\n",
    "    fixed_url_list = channel_db['yt_fixed_url'].to_list()\n",
    "    channel_name_list = channel_db['channel_name'].to_list()\n",
    "\n",
    "    # Loop through each URL, send request, and save response to a file\n",
    "    for yt_nm, url in zip(channel_name_list, fixed_url_list):\n",
    "        payload = {\n",
    "            'api_key': api_key,\n",
    "            'url': url\n",
    "        }\n",
    "        # Send the request\n",
    "        r = requests.get('https://api.scraperapi.com/', params=payload, headers=headers, verify=False)\n",
    "        # r.text는 YT 채널 쇼츠 페이지, 기존에 없던건 list로 저장, 있던 것중 shorts_published_date가 -7d, -14d, -21d에 매칭이 되는게 있다면 list에 추가\n",
    "\n",
    "\n",
    "# scanning_yt_shorts(API_KEY, channel_df, loading_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47912c1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fe1fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading_db = pd.DataFrame(columns = ['channel_nm', 'shorts_url', 'shorts_title', 'shorts_description', 'shorts_published_date', 'shorts_thumbnalil', 'shorts_view', 'check_date'])\n",
    "# loading_db.to_excel('loading_db.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73fbec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3698f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad24f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
