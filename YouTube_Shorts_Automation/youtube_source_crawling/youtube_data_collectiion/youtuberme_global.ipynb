{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d5e917",
   "metadata": {},
   "source": [
    "### YouTuberme channel Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9e7fe",
   "metadata": {},
   "source": [
    "##### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fee6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd6843",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3bff112",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "file_path = f\"DB/Youtube_Data_{today_date}.xlsx\"\n",
    "\n",
    "# List all files that match the pattern \"Youtube_Data_*.xlsx\"\n",
    "files = glob.glob(\"DB/Youtube_Data_*.xlsx\")\n",
    "\n",
    "# Extract dates from filenames and find the latest date\n",
    "dates = [os.path.splitext(os.path.basename(file))[0].replace(\"Youtube_Data_\", \"\") for file in files]\n",
    "dates = sorted(dates, reverse=True)\n",
    "\n",
    "# Define the file name\n",
    "file_name = \"DB/country_category_url.xlsx\"\n",
    "\n",
    "# start url(30 countries): top 1000\n",
    "country_dic = {\"United States\": \"https://us.youtubers.me/united-states/all/top-1000-youtube-channels-in-united-states\",\n",
    "               \"South Korea\": \"https://us.youtubers.me/korea-republic-of/all/top-1000-youtube-channels-in-korea-republic-of\",\n",
    "               \"Germany\":\"https://us.youtubers.me/germany/all/top-1000-youtube-channels-in-germany\",\n",
    "               \"United Kingdom\":\"https://us.youtubers.me/united-kingdom/all/top-1000-youtube-channels-in-united-kingdom\",\n",
    "               \"Brazil\": \"https://us.youtubers.me/brazil/all/top-1000-youtube-channels-in-brazil\",\n",
    "               \"Mexico\" : \"https://us.youtubers.me/mexico/all/top-1000-youtube-channels-in-mexico\",\n",
    "               \"Spain\": \"https://us.youtubers.me/spain/all/top-1000-youtube-channels-in-spain\",\n",
    "               \"Italy\" : \"https://us.youtubers.me/italy/all/top-1000-youtube-channels-in-italy\",\n",
    "               \"Czech Republic\": \"https://us.youtubers.me/czech-republic/all/top-1000-youtube-channels-in-czech-republic\",\n",
    "               \"Russia\":\"https://us.youtubers.me/russian-federation/all/top-1000-youtube-channels-in-russian-federation\",\n",
    "               \"India\" : \"https://us.youtubers.me/india/all/top-1000-youtube-channels-in-india\",\n",
    "               \"France\": \"https://us.youtubers.me/france/all/top-1000-youtube-channels-in-france\",\n",
    "               \"Japan\" : \"https://us.youtubers.me/japan/all/top-1000-youtube-channels-in-japan\",\n",
    "               \"Turkey\": \"https://us.youtubers.me/turkey/all/top-1000-youtube-channels-in-turkey\",\n",
    "               \"Poland\": \"https://us.youtubers.me/poland/all/top-1000-youtube-channels-in-poland\",\n",
    "               \"Canada\" : \"https://us.youtubers.me/canada/all/top-1000-youtube-channels-in-canada\",\n",
    "               \"Vietnam\" : \"https://us.youtubers.me/viet-nam/all/top-1000-youtube-channels-in-viet-nam\",\n",
    "               \"Thailand\" : \"https://us.youtubers.me/thailand/all/top-1000-youtube-channels-in-thailand\",\n",
    "               \"Indonesia\" : \"https://us.youtubers.me/indonesia/all/top-1000-youtube-channels-in-indonesia\",\n",
    "               \"Ukraine\" : \"https://us.youtubers.me/ukraine/all/top-1000-youtube-channels-in-ukraine\",\n",
    "               \"Morocco\" : \"https://us.youtubers.me/morocco/all/top-1000-youtube-channels-in-morocco\",\n",
    "               \"Argentina\" : \"https://us.youtubers.me/argentina/all/top-1000-youtube-channels-in-argentina\",\n",
    "               \"Saudi Arabia\": \"https://us.youtubers.me/saudi-arabia/all/top-1000-youtube-channels-in-saudi-arabia\",\n",
    "               \"Netherlands\": \"https://us.youtubers.me/netherlands/all/top-1000-youtube-channels-in-netherlands\",\n",
    "               \"Egypt\": \"https://us.youtubers.me/egypt/all/top-1000-youtube-channels-in-egypt\",\n",
    "               \"Taiwan\": \"https://us.youtubers.me/taiwan/all/top-1000-youtube-channels-in-taiwan\",\n",
    "               \"Australia\": \"https://us.youtubers.me/australia/all/top-1000-youtube-channels-in-australia\",\n",
    "               \"Greece\": \"https://us.youtubers.me/greece/all/top-1000-youtube-channels-in-greece\",\n",
    "               \"Colombia\": \"https://us.youtubers.me/colombia/all/top-1000-youtube-channels-in-colombia\",\n",
    "               \"Romania\" : \"https://us.youtubers.me/romania/all/top-1000-youtube-channels-in-romania\"\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e595193a",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url 포함 youtuberme df 생성\n",
    "def collect_youtuberme_url(category_url):\n",
    "    page = requests.get(category_url)\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    # Find the table with class \"top-charts\"\n",
    "    table = soup.find(\"table\", class_=\"top-charts\")\n",
    "\n",
    "    # Find all <a> tags within the table\n",
    "    href_list = []\n",
    "    if table:\n",
    "        for a_tag in table.find_all(\"a\"):\n",
    "            href_value = a_tag.get(\"href\")\n",
    "            if href_value.endswith(\"/youtuber-stats\"):\n",
    "                href_list.append(\"https://us.youtubers.me/\" + href_value)\n",
    "    return href_list\n",
    "# 여러 카테고리 확인\n",
    "def crawl_table_urls_extracted(start_url, table_class='top-charts', depth=1):\n",
    "    visited_urls = set()\n",
    "    excluded_urls = []\n",
    "    extracted_strings = {}\n",
    "\n",
    "    def extract_string_between_substrings(url, start_substring, end_substring):\n",
    "        start_index = url.find(start_substring)\n",
    "        end_index = url.find(end_substring, start_index + len(start_substring))\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            return url[start_index + len(start_substring):end_index]\n",
    "        return None\n",
    "\n",
    "    def recursive_crawl(url, current_depth):\n",
    "        if current_depth > depth:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # Find the table with the specified class\n",
    "                table = soup.find('table', class_=table_class)\n",
    "\n",
    "                if table:\n",
    "                    # Extract href attributes from anchor tags within the table\n",
    "                    for row in table.find_all('tr'):\n",
    "                        columns = row.find_all('td')\n",
    "                        for col_index, col in enumerate(columns):\n",
    "                            # Exclude href attributes from the column with the name \"category\"\n",
    "                            if col.get_text(strip=True).lower() == 'category':\n",
    "                                continue\n",
    "\n",
    "                            hrefs = [a.get('href') for a in col.find_all('a', href=True)]\n",
    "\n",
    "                            # Process the hrefs\n",
    "                            for href in hrefs:\n",
    "                                absolute_url = urljoin(url, href)\n",
    "\n",
    "                                # Save URLs without \"korea-republic-of\"\n",
    "                                if start_url[-8:] not in absolute_url.lower():\n",
    "                                    excluded_urls.append(absolute_url)\n",
    "                                else:\n",
    "                                    visited_urls.add(absolute_url)\n",
    "\n",
    "                                    # Extract and save strings between specified substrings\n",
    "                                    extracted_string = extract_string_between_substrings(\n",
    "                                        absolute_url,\n",
    "                                        start_url[:28],\n",
    "                                        '/top-1000-'\n",
    "                                    )\n",
    "                                    if extracted_string:\n",
    "                                        # Create a set for each visited URL to remove duplicates\n",
    "                                        extracted_strings.setdefault(absolute_url, set()).add(extracted_string)\n",
    "\n",
    "                                    # Avoid revisiting the same URL\n",
    "                                    if absolute_url not in visited_urls:\n",
    "                                        # Recursively crawl the next level\n",
    "                                        recursive_crawl(absolute_url, current_depth + 1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {url}: {e}\")\n",
    "\n",
    "    recursive_crawl(start_url, 1)\n",
    "\n",
    "    # Convert sets to lists\n",
    "    excluded_urls = list(set(excluded_urls))\n",
    "    visited_urls = list(set(visited_urls))\n",
    "\n",
    "    # Convert sets to lists within the extracted_strings dictionary\n",
    "    for url, strings_set in extracted_strings.items():\n",
    "        extracted_strings[url] = list(strings_set)\n",
    "    updated_data = {}\n",
    "    for key, value in extracted_strings.items():\n",
    "        split_value = value[0].split('/')\n",
    "        if len(split_value) >= 2:\n",
    "            updated_value = split_value[1]\n",
    "            updated_data[key] = [updated_value]\n",
    "    return updated_data\n",
    "\n",
    "    \n",
    "# category 통일(union 편하게)\n",
    "def category_preprocessing(df):\n",
    "    df['category'] = df['category'].replace('nan', 'all')\n",
    "\n",
    "    # Mapping of old categories to new categories\n",
    "    category_mapping = {'Film & Animation': 'film-animation',\n",
    "                        'Autos & Vehicles': 'autos-vehicles',\n",
    "                        'Music': 'music',\n",
    "                        'Movies': 'movies',\n",
    "                        'Pets & Animals': 'pets-animals',\n",
    "                        'Sports': 'sports',\n",
    "                        'Travel & Events': 'travel-events',\n",
    "                        'Gaming': 'gaming',\n",
    "                        'People & Blogs': 'people-blogs',\n",
    "                        'Comedy': 'comedy',\n",
    "                        'Entertainment': 'entertainment',\n",
    "                        'News & Politics': 'news-politics',\n",
    "                        'Howto & Style': 'howto-style',\n",
    "                        'Education': 'education',\n",
    "                        'Science & Technology': 'science-technology',\n",
    "                        'Shows': 'shows',\n",
    "                        'Nonprofits & Activism': 'nonprofits-activism',\n",
    "                        'all': 'all'}\n",
    "\n",
    "    # Map the old categories to the new categories\n",
    "    df['category'] = df['category'].map(category_mapping)\n",
    "    return df\n",
    "\n",
    "# category df 생성\n",
    "def create_dataframe(extracted_strings, country):\n",
    "    data = {'url': [], 'category': [], 'country' : []}\n",
    "    for url, categories in extracted_strings.items():\n",
    "        for category in categories:\n",
    "            data['url'].append(url)\n",
    "            data['category'].append(category)\n",
    "            data['country'].append(country)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# youtuberme df 생성\n",
    "def collect_youtuberme_basic(url, country):\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "    # 현재 페이지에서 table 태그 모두 선택하기\n",
    "    table1 = soup.select('table')\n",
    "\n",
    "    # 하나의 테이블 태그 선택하기\n",
    "    table = table1[0]\n",
    "\n",
    "    df_top1000 = pd.read_html(str(table))[0]\n",
    "    href_list = collect_youtuberme_url(url)\n",
    "    df_top1000['url'] = href_list\n",
    "    df_top1000['country'] = [country for x in range(len(df_top1000))]\n",
    "    # Check if the first category value is NaN, and assign the second value if it is\n",
    "    if pd.isna(df_top1000['category'].iloc[0]):\n",
    "        df_top1000['category'].iloc[0] = df_top1000['category'].iloc[1]\n",
    "    # Now, you can assign the value to the entire column as per your requirement\n",
    "    df_top1000['category'] = df_top1000['category'].iloc[0]    \n",
    "    # print(\"url_list: \", len(href_list))\n",
    "    df_top1000 = category_preprocessing(df_top1000)\n",
    "    return df_top1000\n",
    "\n",
    "# 기존 + 신규 데이터 추가\n",
    "def update_channels(past_df, df):\n",
    "    combined_df = pd.concat([past_df, df])\n",
    "    # Drop duplicates based on 'channel_name' and keep only the unique entries\n",
    "    combined_df = combined_df.drop_duplicates(subset=['channel_name'])\n",
    "    return combined_df\n",
    "\n",
    "# excute whole process\n",
    "def excute_youtuberme_crawling(county_category_data, past_df):#,country_url_df\n",
    "    # 전체 카테고리 별 채널 수집\n",
    "    # country_category_df = pd.read_excel(\"country_category_url.xlsx\")\n",
    "    # url 돌면서 필요한 데이터 \n",
    "    country_url_df = []\n",
    "    for url, country in zip(county_category_data['url'].to_list(), county_category_data['country'].to_list()):    \n",
    "        df_new = collect_youtuberme_basic(url, country)\n",
    "        country_url_df.append(df_new)\n",
    "        df = pd.concat(country_url_df, axis=0, ignore_index=True)\n",
    "    # 중복 제거\n",
    "    df = df.drop_duplicates('Youtuber')\n",
    "    df = df.drop(\"rank\", axis=1)\n",
    "    # YouTube URL 저장\n",
    "    df['youtube url'] = [url[:-7] for url in  df['url'].to_list()]\n",
    "    df['yt_fixed_url'] = ''\n",
    "    df['data_yn'] = 'y'    \n",
    "    # column명 변경\n",
    "    df = df.rename(columns={\n",
    "        'url': 'youtuberme_url',\n",
    "        'Youtuber': 'channel_name',\n",
    "        'video views': 'total_video_views',\n",
    "        'video count': 'total_video_count',\n",
    "        'youtube url': 'yt_url'\n",
    "    })\n",
    "    # subscribers, total_video_count가 0인 경우 채널이 삭제된 경우이므로 제거\n",
    "    df = df[(df['subscribers'] != 0) & (df['subscribers'].notna())]\n",
    "    df = update_channels(df, past_df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cddeb7",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: DB/Youtube_Data_2024-10-09.xlsx\n"
     ]
    }
   ],
   "source": [
    "if dates:\n",
    "    # Load the latest file based on the date\n",
    "    latest_file = f\"DB/Youtube_Data_{dates[0]}.xlsx\"\n",
    "    previous_df = pd.read_excel(latest_file)\n",
    "    print(f\"Loaded file: {latest_file}\")\n",
    "else:\n",
    "    print(\"No files found.\")\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Load the Excel file into a DataFrame\n",
    "    country_category_df = pd.read_excel(file_name)\n",
    "else:\n",
    "    print(f\"{file_name} does not exist. The code will not run.\")\n",
    "    # category별 url 리스트 합치기\n",
    "    dfs = []\n",
    "    for country, url in country_dic.items():\n",
    "        print(country, url)\n",
    "        extracted_strings = crawl_table_urls_extracted(url, table_class='top-charts', depth=1)\n",
    "        df_category = create_dataframe(extracted_strings, country)\n",
    "        dfs.append(df_category)\n",
    "        country_category_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "        print(country_category_df)\n",
    "        # 전체 URL 저장\n",
    "        country_category_df.to_excel(\"DB/country_category_url.xlsx\", index=False)\n",
    "\n",
    "# 우선은 한국 미국만\n",
    "country_category_df = country_category_df[country_category_df['country'].isin(['South Korea', 'United States'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9dfd9a",
   "metadata": {},
   "source": [
    "##### YouTuberme Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74c47777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n",
      "C:\\Users\\HyunJunLee\\AppData\\Local\\Temp\\ipykernel_34704\\3349003966.py:151: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_top1000 = pd.read_html(str(table))[0]\n"
     ]
    }
   ],
   "source": [
    "result_df = excute_youtuberme_crawling(country_category_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22624a7e",
   "metadata": {},
   "source": [
    "##### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44d8849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_excel(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
