{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b221f3-f89b-485e-980d-a3aa40636c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files already exist in the output folder. Skipping splitting.\n",
      "Selected CSV file: split-csv\\split_1.csv\n",
      "Cookies added\n",
      "CSV file 'successful_Data\\data-18-05-2024.csv' already exists.\n",
      "CSV file 'failed_Data\\No_Short_Videos_Or_Banned.csv' already exists.\n",
      "CSV file 'failed_Data\\No_Short_Video_in_date-Range-18-05-2024.csv' already exists.\n",
      "18-05-2024\n",
      "Processing link  ##  https://us.youtubers.me//jinho-huh/youtube\n",
      "loading url\n",
      "Subscribers: 11600\n",
      "Channel Name: 대추씨\n",
      "Playlists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import undetected_chromedriver as uc\n",
    "import os\n",
    "import csv\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "# Import the necessary module\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "# Define the range for the random sleep time in seconds\n",
    "min_sleep_time, max_sleep_time = 1, 3\n",
    "\n",
    "# Generate and use a random sleep time within the defined range\n",
    "random_sleep=time.sleep(random.uniform(min_sleep_time, max_sleep_time))\n",
    "\n",
    "\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.headless = False\n",
    "\n",
    "driver = uc.Chrome(options=options, use_subprocess=True)\n",
    "# Set the browser window size\n",
    "driver.set_window_size(1000, 800)\n",
    "####################################################################\n",
    "\n",
    "def check_folder_empty(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    return not any(filename.endswith('.csv') for filename in os.listdir(folder))\n",
    "\n",
    "def split_csv(input_file, output_folder, max_records=500):\n",
    "    if not check_folder_empty(output_folder):\n",
    "        print(\"CSV files already exist in the output folder. Skipping splitting.\")\n",
    "        return\n",
    "\n",
    "    with open(input_file, 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)  # Read the header\n",
    "\n",
    "        count = 0\n",
    "        file_index = 1\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        output_file = os.path.join(output_folder, f'split_{file_index}.csv')\n",
    "        with open(output_file, 'w', newline='') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerow(header)  # Write the header to each split file\n",
    "            for row in reader:\n",
    "                writer.writerow(row)\n",
    "                count += 1\n",
    "                if count == max_records:\n",
    "                    count = 0\n",
    "                    file_index += 1\n",
    "                    output_file = os.path.join(output_folder, f'split_{file_index}.csv')\n",
    "                    outfile.close()  # Close the current file\n",
    "                    outfile = open(output_file, 'w', newline='')  # Reopen the file\n",
    "                    writer = csv.writer(outfile)\n",
    "                    writer.writerow(header)\n",
    "\n",
    "\n",
    "def crawler(output_folder, csv_file_index):\n",
    "    csv_files = [filename for filename in os.listdir(output_folder) if filename.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in the split folder.\")\n",
    "        return None\n",
    "    else:\n",
    "        if csv_file_index < 1 or csv_file_index > len(csv_files):\n",
    "            print(\"Invalid CSV file index.\")\n",
    "            return None\n",
    "        else:\n",
    "            return os.path.join(output_folder, csv_files[csv_file_index - 1])\n",
    "\n",
    "# Example usage:\n",
    "input_file = 'links.csv'  # Replace 'input.csv' with your CSV file path\n",
    "output_folder = 'split-csv'  # Folder where split CSV files will be saved\n",
    "split_csv(input_file, output_folder, max_records=500)\n",
    "\n",
    "# Select a CSV file from the split folder\n",
    "csv_link_file =  crawler(output_folder, csv_file_index=1)\n",
    "print(\"Selected CSV file:\", csv_link_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def convert_subscriber_count(subscriber_str):\n",
    "    match = re.match(r\"(\\d+(\\.\\d+)?)([MK]?)\", subscriber_str, re.IGNORECASE)\n",
    "    if match:\n",
    "        number = float(match.group(1))\n",
    "        suffix = match.group(3).upper()\n",
    "\n",
    "        if suffix == 'M':\n",
    "            return int(number * 1_000_000)\n",
    "        elif suffix == 'K':\n",
    "            return int(number * 1_000)\n",
    "        else:\n",
    "            return int(number)  # No suffix, return the number as is\n",
    "    return int(subscriber_str)  # If input format is unexpected, try returning it as integer\n",
    "\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "def login(driver, cookies_file_path='cookies.pkl'):\n",
    "    if os.path.exists(cookies_file_path):\n",
    "        # Load cookies if the file exists\n",
    "        with open(cookies_file_path, 'rb') as cookies_file:\n",
    "            cookies = pickle.load(cookies_file)\n",
    "            # Use the loaded cookies for the session\n",
    "            driver.get('https://www.youtube.com')\n",
    "            driver.implicitly_wait(10)\n",
    "            for cookie in cookies:\n",
    "                driver.add_cookie(cookie)\n",
    "            print(\"Cookies added\")\n",
    "            driver.refresh()\n",
    "            driver.implicitly_wait(10)\n",
    "    else:\n",
    "\n",
    "        driver.get('https://www.youtube.com')\n",
    "        driver.implicitly_wait(15)\n",
    "\n",
    "\n",
    "        try:\n",
    "            clk=driver.find_element(By.XPATH,\"(//div[@class='yt-spec-touch-feedback-shape yt-spec-touch-feedback-shape--touch-response-inverse'])[2]\").click()\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            driver.implicitly_wait(10)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            driver.find_element(By.XPATH,\"//*[@id='buttons']/ytd-button-renderer/yt-button-shape/a/yt-touch-feedback-shape/div\").click()\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            driver.implicitly_wait(10)\n",
    "            element=driver.find_element(By.XPATH,\"//*[@id='identifierId']\").click()\n",
    "            print(\"cLICKED\")\n",
    "            # Find and input the email/username\n",
    "            email_field = driver.find_element(By.XPATH, \"//*[@id='identifierId']\")\n",
    "            email_field.send_keys(\"darkjhon449\")  # Replace with your email/username\n",
    "            time.sleep(random.uniform(1, 3))  # Adding a delay to ensure the text is inputted before continuing\n",
    "\n",
    "            # Press Enter to proceed to the password input\n",
    "            email_field.send_keys(Keys.ENTER)\n",
    "            time.sleep(random.uniform(1, 3))  # Adding a delay to ensure the page loads properly\n",
    "            driver.implicitly_wait(10)\n",
    "\n",
    "            email_pass = driver.find_element(By.XPATH, \"//*[@id='password']/div[1]/div/div[1]/input\")\n",
    "            email_pass.send_keys(\"Pakistan@1947\")  # Replace with your email/username\n",
    "            time.sleep(1)  # Adding a delay to ensure the text is inputted before continuing\n",
    "\n",
    "            # Press Enter to proceed to the password input\n",
    "            email_pass.send_keys(Keys.ENTER)\n",
    "            time.sleep(random.uniform(1, 3)) # Adding a delay to ensure the page loads properly\n",
    "            driver.implicitly_wait(10)\n",
    "            input()\n",
    "\n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Save cookies for future use\n",
    "        with open(cookies_file_path, 'wb') as cookies_file:\n",
    "            pickle.dump(driver.get_cookies(), cookies_file)\n",
    "        print(\"cookies saved\")\n",
    "\n",
    "login(driver=driver)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_csv_files():\n",
    "    # Create a folder named \"successful_Data\" if it doesn't exist\n",
    "    headers = [\"chanel_name\", \"yt_url\", \"subscribers\", \"check_date\", \"released_date\", \"is_posted_in_date_range\",\n",
    "               \"shorts_name\", \"description\", \"likes\", \"views\", \"short_url\"]\n",
    "    headers2 = [\"yt_url\", \"check_date\", \"is_posted_in_date_range\"]\n",
    "\n",
    "    folder_name = \"successful_Data\"\n",
    "    folder_name2 = \"failed_Data\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        print(f\"Folder '{folder_name}' created successfully.\")\n",
    "\n",
    "    if not os.path.exists(folder_name2):\n",
    "        os.makedirs(folder_name2)\n",
    "        print(f\"Folder '{folder_name2}' created successfully.\")\n",
    "\n",
    "    # Get the current date in the desired format\n",
    "    current_date = datetime.now().strftime(\"-%d-%m-%Y\")\n",
    "    file_name = os.path.join(folder_name, f\"data{current_date}.csv\")\n",
    "\n",
    "    # Check if a file with the current date exists\n",
    "    if not os.path.exists(file_name):\n",
    "        # If it doesn't exist, create a new CSV file with that name\n",
    "\n",
    "        pd.DataFrame(columns=headers).to_csv(file_name, index=False)\n",
    "        print(f\"CSV file '{file_name}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"CSV file '{file_name}' already exists.\")\n",
    "\n",
    "    # Create CSV files in \"failed_Data\" folder\n",
    "    failed_folder_name = \"failed_Data\"\n",
    "    Banned_or_no_short_video = os.path.join(failed_folder_name, \"No_Short_Videos_Or_Banned.csv\")\n",
    "    no_video_in_date_range = os.path.join(failed_folder_name, f\"No_Short_Video_in_date-Range{current_date}.csv\")\n",
    "\n",
    "    # Check if the files already exist\n",
    "    if not os.path.exists(Banned_or_no_short_video):\n",
    "        # Create the file only if it doesn't exist\n",
    "        pd.DataFrame(columns=headers2).to_csv(Banned_or_no_short_video, index=False)\n",
    "        print(f\"CSV file '{Banned_or_no_short_video}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"CSV file '{Banned_or_no_short_video}' already exists.\")\n",
    "\n",
    "    if not os.path.exists(no_video_in_date_range):\n",
    "        # Create the file only if it doesn't exist\n",
    "        pd.DataFrame(columns=headers).to_csv(no_video_in_date_range, index=False)\n",
    "        print(f\"CSV file '{no_video_in_date_range}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"CSV file '{no_video_in_date_range}' already exists.\")\n",
    "\n",
    "    return file_name, Banned_or_no_short_video, no_video_in_date_range\n",
    "\n",
    "csv_file, Banned_or_no_short_video1, no_video_in_date_range1 = create_csv_files()\n",
    "\n",
    "\n",
    "\n",
    "###################################\n",
    "headers = [\"chanel_name\", \"yt_url\", \"subscribers\", \"check_date\", \"released_date\", \"is_posted_in_date_range\",\n",
    "           \"shorts_name\", \"description\", \"likes\", \"views\", \"short_url\"]\n",
    "\n",
    "df = pd.DataFrame(columns=headers)\n",
    "check_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "print(check_date)\n",
    "############################################\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "\n",
    "        # Read channel URLs from the CSV file\n",
    "        with open(csv_link_file, \"r\") as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            rows = list(reader)  # Convert the reader object to a list of rows for easier access\n",
    "            for index, row in enumerate(rows):\n",
    "                check_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "                url = row['links']\n",
    "                processed = row['processed']\n",
    "                if not processed:  # Process only if 'processed' column is empty\n",
    "                    # print(url)\n",
    "                    chanel_url = url  # Store the URL in chanel_url variable\n",
    "                    print(f\"Processing link  ##  \" + url)\n",
    "\n",
    "                    print(\"loading url\")\n",
    "                    # Load the channel URL in the browser\n",
    "                    driver.get(url.strip())  # Strip any leading/trailing whitespaces\n",
    "                    driver.implicitly_wait(10)\n",
    "                    fixed_url = driver.current_url\n",
    "\n",
    "                    # Update the processed column for the current URL to 'Yes'\n",
    "                    rows[index]['processed'] = 'Yes'\n",
    "\n",
    "                    # Write back to the CSV file with updated data\n",
    "                    with open(csv_link_file, \"w\", newline='') as csvfile:\n",
    "                        fieldnames = ['links', 'processed']\n",
    "                        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                        writer.writeheader()\n",
    "                        writer.writerows(rows)\n",
    "                     # Break the loop after processing the current URL\n",
    "\n",
    "                    time.sleep(2)\n",
    "\n",
    "                    # Get page source\n",
    "                    page_source = driver.page_source\n",
    "\n",
    "                    # Parse page source using BeautifulSoup\n",
    "                    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                    # Extract all text from the page\n",
    "                    all_text = soup.get_text()\n",
    "                    # print(all_text)\n",
    "\n",
    "                    # Using regular expression to extract subscribers count\n",
    "                    subscribers_pattern = re.search(r'(\\d+(?:\\.\\d+)?[KM]?)(?=\\s*subscribers)', all_text, re.IGNORECASE)\n",
    "                    if subscribers_pattern:\n",
    "                        subscribers = subscribers_pattern.group().strip()\n",
    "                        #############################\n",
    "\n",
    "\n",
    "                        subscriber_str =subscribers\n",
    "\n",
    "\n",
    "                        subscribers = convert_subscriber_count(subscriber_str)\n",
    "\n",
    "                        print(\"Subscribers:\", subscribers)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        subscribers=\"\"\n",
    "                        print(\"Subscribers not found.\")\n",
    "\n",
    "                    # Using regular expression to extract channel name\n",
    "                    channel_pattern = re.search(r'(.+)(?= - YouTube)', all_text)\n",
    "                    if channel_pattern:\n",
    "                        channel_name = channel_pattern.group().strip()\n",
    "                        print(\"Channel Name:\", channel_name)\n",
    "                    else:\n",
    "                        channel_name=\"\"\n",
    "                        print(\"Channel name not found.\")\n",
    "\n",
    "\n",
    "                    if (\n",
    "                            \" terminated because we received multiple third-party claims of copyright\" in all_text or \"This channel is not available\" in all_text or \"This channel does not exist\" in all_text or \"The page you were looking for doesn't exist\" in all_text or\n",
    "                            \"account has been terminated because it is linked to an account that received multiple third-party claims of copyright infringement\" in all_text) or \"channel was removed because it violated our Community Guidelines\" in all_text:\n",
    "\n",
    "                        print(\"banned account\")\n",
    "                        #saving data\n",
    "                        new_row = {'yt_url': fixed_url,\n",
    "                                   'check_date': check_date,\n",
    "                                   'is_posted_in_date_range': \"Banned\"}\n",
    "\n",
    "                        ##################################################################################################################################\n",
    "                        print(\"Trying 1\")\n",
    "                        ############################################\n",
    "                        fieldnames = ['yt_url', 'check_date', 'is_posted_in_date_range']\n",
    "\n",
    "                        # Check if the file is empty\n",
    "                        with open(Banned_or_no_short_video1, 'a', encoding='utf-8', newline='') as csvfile:\n",
    "                            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                            writer.writerow(new_row)  # Append the new row\n",
    "\n",
    "\n",
    "\n",
    "                        print(\"New row appended successfully.\")\n",
    "\n",
    "                        print(\"data is saved\")\n",
    "                        break\n",
    "                    ##############################################\n",
    "                    # Get the page source\n",
    "                    page_source = driver.page_source\n",
    "\n",
    "                    # Parse the page source using BeautifulSoup\n",
    "                    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                    # Find the element matching the XPath\n",
    "                    element = soup.find(\"yt-tab-shape\", {\"tab-title\": \"Shorts\"})\n",
    "\n",
    "                    #######################################\n",
    "                    find_short = None\n",
    "                    short_xpath = \"//*[@id='tabsContent']/yt-tab-group-shape/div[1]/yt-tab-shape[{}]\"\n",
    "\n",
    "                    for i in range(1, 4):  # Try different indexes from 1 to 9\n",
    "                        try:\n",
    "                            short_xpath_formatted = short_xpath.format(i)\n",
    "                            find_short = driver.find_element(By.XPATH, short_xpath_formatted).text\n",
    "                            if \"Shorts\" in find_short:\n",
    "                                print(f\"Found 'Shorts' at XPath: {short_xpath_formatted}\")\n",
    "                                click_short = driver.find_element(By.XPATH, short_xpath_formatted).click()\n",
    "                                break\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    if find_short is None:\n",
    "                        print(\"Text 'Shorts' not found in any of the XPaths.\")\n",
    "                        # saving data\n",
    "                        new_row = {'yt_url': fixed_url,\n",
    "                                   'check_date': check_date,\n",
    "                                   'is_posted_in_date_range': \"No short Videos\"}\n",
    "\n",
    "                        ##################################################################################################################################\n",
    "                        print(\"Trying 2\")\n",
    "                        ############################################\n",
    "                        # Define the header and new row\n",
    "                        fieldnames = ['yt_url', 'check_date',\n",
    "                                      'is_posted_in_date_range']\n",
    "\n",
    "                        # Check if the file is empty\n",
    "\n",
    "                        with open(Banned_or_no_short_video1, 'a', encoding='utf-8', newline='') as csvfile:\n",
    "                            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                            writer.writerow(new_row)  # Append the new row\n",
    "\n",
    "                        print(\"New row appended successfully.\")\n",
    "\n",
    "                        print(\"data is saved\")\n",
    "\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                        print(find_short)\n",
    "\n",
    "\n",
    "                    shorts = driver.find_elements(By.XPATH,\n",
    "                                                  \"//a[@class='yt-simple-endpoint focus-on-expand style-scope ytd-rich-grid-slim-media']\")\n",
    "                    print(len(shorts))\n",
    "\n",
    "                    if shorts is None or len(shorts) < 1:\n",
    "\n",
    "                        print(\"No Short Videos loop breaked\")\n",
    "                        # saving data\n",
    "                        new_row = {'yt_url': fixed_url,\n",
    "                                   'check_date': check_date,\n",
    "                                   'is_posted_in_date_range': \"No short Videos\"}\n",
    "\n",
    "                        ##################################################################################################################################\n",
    "                        print(\"Trying 3\")\n",
    "                        ############################################\n",
    "                        # Define the header and new row\n",
    "                        fieldnames = ['yt_url', 'check_date',\n",
    "                                      'is_posted_in_date_range']\n",
    "\n",
    "                        # Check if the file is empty\n",
    "                        with open(Banned_or_no_short_video1, 'a', encoding='utf-8', newline='') as csvfile:\n",
    "                            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                            writer.writerow(new_row)  # Append the new row\n",
    "\n",
    "\n",
    "\n",
    "                        print(\"New row appended successfully.\")\n",
    "                        print(\"data is saved\")\n",
    "\n",
    "\n",
    "                        print(\n",
    "                            \"33333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333\")\n",
    "                        break\n",
    "\n",
    "                    # Store links in a list\n",
    "                    links = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    for short in shorts:\n",
    "                        link = short.get_attribute(\"href\")\n",
    "                        links.append(link)\n",
    "                        # print(link)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    print(\"all links saved\")\n",
    "                    # Set a flag to keep track of whether the 'if' part of the loop has executed\n",
    "                    if_part_executed = False\n",
    "                    # Iterate over the links and load them one by one\n",
    "\n",
    "                    for link in links:\n",
    "                        print(\"Loading link:\", link)\n",
    "                        driver.get(link)\n",
    "                        driver.implicitly_wait(10)\n",
    "                        try:\n",
    "\n",
    "                            short_name = driver.find_element(By.XPATH,\n",
    "                                                             \"//h2[@class='title style-scope reel-player-header-renderer']\").text\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                        print(short_name)\n",
    "\n",
    "                        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "                       # print(\"click1\")\n",
    "                        clk = driver.find_element(By.XPATH,\n",
    "                                                 \"//*[@id='button-shape']/button/yt-touch-feedback-shape/div/div[2]\").click()\n",
    "                        #print(\"click2\")\n",
    "                        time.sleep(random.uniform(1, 3))\n",
    "                        clk2 = driver.find_element(By.XPATH,\"//*[@id='contentWrapper']/ytd-menu-popup-renderer/tp-yt-paper-listbox/ytd-menu-service-item-renderer\").click()\n",
    "                                                 #  \"//*[@id='items']/ytd-menu-service-item-renderer/tp-yt-paper-item/yt-formatted-string\").click()\n",
    "                        print(\"clicked 3\")\n",
    "\n",
    "                        description_text = driver.find_element(By.XPATH,\n",
    "                                                               \"//*[@id='watch-while-engagement-panel']/ytd-engagement-panel-section-list-renderer[2]\").text\n",
    "\n",
    "                        #print(description_text)\n",
    "\n",
    "\n",
    "                        text = description_text.replace(\"Description\", \"\")\n",
    "                        text = text.strip()  # To remove any leading or trailing whitespace\n",
    "\n",
    "                        # Split the text into lines\n",
    "                        lines = text.strip().split('\\n')\n",
    "\n",
    "                        # Find the index of the line containing 'Views'\n",
    "                        views_index = -1\n",
    "                        for i, line in enumerate(lines):\n",
    "                            if 'Views' in line:\n",
    "                                views_index = i\n",
    "                                break\n",
    "\n",
    "                        # Remove the 'Views' line, the three lines above it, and the two lines below it\n",
    "                        if views_index >= 3 and views_index + 2 < len(lines):\n",
    "                            lines = lines[:views_index - 3] + lines[views_index + 2 + 1:]\n",
    "\n",
    "                        # Join the remaining lines back into a single string\n",
    "                        cleaned_text = '\\n'.join(lines)\n",
    "\n",
    "                        description = cleaned_text\n",
    "                        print(\"description:::::  \", cleaned_text)\n",
    "\n",
    "                        ###########################\n",
    "                        # Split the text into lines\n",
    "                        lines = text.strip().split('\\n')\n",
    "\n",
    "                        # Find the index of the line containing 'Likes'\n",
    "                        likes_index = -1\n",
    "                        for i, line in enumerate(lines):\n",
    "                            if 'Likes' in line:\n",
    "                                likes_index = i\n",
    "                                break\n",
    "\n",
    "                        # Save the line above 'Likes' as the likes variable\n",
    "                        if likes_index > 0:\n",
    "                            likes = lines[likes_index - 1]\n",
    "                        else:\n",
    "                            likes = None  # Handle the case where there is no line above 'Likes'\n",
    "\n",
    "                        # Output the likes variable\n",
    "                        print(f\"Likes are : {likes}\")\n",
    "\n",
    "                        # Find the index of the line containing 'Views'\n",
    "                        Views_index = -1\n",
    "                        for i, line in enumerate(lines):\n",
    "                            if 'Views' in line:\n",
    "                                Views_index = i\n",
    "                                break\n",
    "\n",
    "                        # Save the line above 'Likes' as the likes variable\n",
    "                        if Views_index > 0:\n",
    "                            veiws = lines[Views_index - 1]\n",
    "                        else:\n",
    "                            veiws = None  # Handle the case where there is no line above 'Likes'\n",
    "\n",
    "                        # Output the likes variable\n",
    "                        print(f\"Views are : {veiws}\")\n",
    "\n",
    "                        # Split the text into lines\n",
    "                        lines = text.strip().split('\\n')\n",
    "\n",
    "                        # Find the index of the line containing 'Views'\n",
    "                        views_index = -1\n",
    "                        for i, line in enumerate(lines):\n",
    "                            if 'Views' in line:\n",
    "                                views_index = i\n",
    "                                break\n",
    "\n",
    "                        # Save the two lines below 'Views'\n",
    "                        if views_index != -1 and views_index + 2 < len(lines):\n",
    "                            below_views = lines[views_index + 1:views_index + 3]\n",
    "                        else:\n",
    "                            below_views = []  # Handle the case where there are fewer than two lines below 'Views'\n",
    "\n",
    "                        # Join the lines below 'Views' into a single string\n",
    "                        below_views_combined = ' '.join(below_views)\n",
    "                        date_str = below_views_combined\n",
    "\n",
    "                        # Output the combined lines\n",
    "                        print(f\"date is ::::  {below_views_combined}\")\n",
    "\n",
    "                        time.sleep(1)\n",
    "                        print(\"+\" * 100)\n",
    "\n",
    "\n",
    "\n",
    "                        try:\n",
    "                            # Try parsing the date assuming it's in the format \"Apr 26 2024\"\n",
    "                            date_obj = datetime.strptime(date_str, \"%b %d %Y\")\n",
    "                        except ValueError:\n",
    "                            # If ValueError occurs, try parsing the date assuming it's in the format \"26 Apr 2024\"\n",
    "                            date_obj = datetime.strptime(date_str, \"%d %b %Y\")\n",
    "                            # try:\n",
    "                            #\n",
    "                            #\n",
    "                            # except ValueError:\n",
    "                            #     # If ValueError occurs again, try parsing the date assuming it's in the format \"2022 Dec 19\"\n",
    "                            #     date_obj = datetime.datetime.strptime(date_str, \"%Y %b %d\")\n",
    "\n",
    "                        # Format the date consistently\n",
    "                        date = date_obj.strftime(\"%b %d %Y\")\n",
    "                        print(\"date   :::\" , date)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        # Get the current date\n",
    "                        current_date = datetime.now()\n",
    "\n",
    "                        # Generate a list of date strings for the past 7 days\n",
    "                        date_range = [(current_date - timedelta(days=i)).strftime(\"%b %d %Y\") for i in range(7)]\n",
    "                        # Print the generated date range\n",
    "                        # print(\"Date Range (Past 7 days):\")\n",
    "\n",
    "                        # for dates in date_range:\n",
    "                        #     print(dates)\n",
    "\n",
    "\n",
    "                        # Check if the date falls within the past 7 days\n",
    "                        if (date in date_range) or (\"ago\" in date.lower()):\n",
    "\n",
    "                            if_part_executed = True\n",
    "\n",
    "\n",
    "\n",
    "                            new_row = {'chanel_name': channel_name, 'yt_url': fixed_url,\n",
    "                                       'subscribers': subscribers,\n",
    "                                       'check_date': check_date, 'released_date': date,\n",
    "                                       'is_posted_in_date_range': \"Yes\", 'shorts_name': short_name,\n",
    "                                       'description': description, 'likes': likes, 'views': veiws,\n",
    "                                       'short_url': link}\n",
    "                            print(new_row)\n",
    "                            #time.sleep(5)\n",
    "\n",
    "                            ##################################################################################################################################\n",
    "                            print(\"Trying 4\")\n",
    "                            ############################################\n",
    "\n",
    "                            # Define the header and new row\n",
    "                            fieldnames = ['chanel_name', 'yt_url', 'subscribers', 'check_date', 'released_date',\n",
    "                                          'is_posted_in_date_range', 'shorts_name', 'description', 'likes', 'views',\n",
    "                                          'short_url']\n",
    "\n",
    "                            with open(csv_file, 'a', encoding='utf-8', newline='') as csvfile:\n",
    "                                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                                writer.writerow(new_row)  # Append the new row\n",
    "\n",
    "\n",
    "\n",
    "                            print(\"New row appended successfully.\")\n",
    "\n",
    "                            print(\"data is saved\")\n",
    "\n",
    "                            print(\"loop is continueing\")\n",
    "\n",
    "\n",
    "                            print(\"%\" * 100)\n",
    "                            print(\n",
    "                                \"4444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\")\n",
    "\n",
    "                        else:\n",
    "                            if not if_part_executed:  # If the 'if' part hasn't executed in the first iteration\n",
    "\n",
    "                                check_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "                                new_row = {'chanel_name': channel_name, 'yt_url': fixed_url,\n",
    "                                           'subscribers': subscribers,\n",
    "                                           'check_date': check_date, 'released_date': date,\n",
    "                                           'is_posted_in_date_range': \"No\",\n",
    "                                           'shorts_name': short_name,\n",
    "                                           'description': description,\n",
    "                                           'likes': likes, 'views': veiws, 'short_url': link}\n",
    "                                print(new_row)\n",
    "                                #time.sleep(5)\n",
    "                                print(\"Trying 5\")\n",
    "                                ############################################\n",
    "                                # Define the header and new row\n",
    "                                fieldnames = ['chanel_name', 'yt_url', 'subscribers', 'check_date', 'released_date',\n",
    "                                              'is_posted_in_date_range', 'shorts_name', 'description', 'likes', 'views',\n",
    "                                              'short_url']\n",
    "\n",
    "                                with open(no_video_in_date_range1, 'a', encoding='utf-8', newline='') as csvfile:\n",
    "                                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                                    writer.writerow(new_row)  # Append the new row\n",
    "\n",
    "                                print(\"New row appended successfully.\")\n",
    "                                print(\"data is saved\")\n",
    "\n",
    "\n",
    "                                print(\"loop breaked\")\n",
    "\n",
    "                                print(\n",
    "                                    \"555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\")\n",
    "\n",
    "                                break\n",
    "                            if (date not in date_range) or (\"ago\"  not in date.lower()):\n",
    "                                print(\"breaking loop after saving data\")\n",
    "                                time.sleep(3)\n",
    "\n",
    "                                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        #input()\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d363b85-cbbc-4f4f-9ef8-6cf3eab96d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
